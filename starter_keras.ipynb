{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "starter-keras.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5ZirafOYJdg"
      },
      "source": [
        "![header](https://i.imgur.com/sAPM7Yy.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9MDnS5kwVze"
      },
      "source": [
        "# Instructions and Starter Code for the DAVIS Contest - Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC-lQXFNYNm8"
      },
      "source": [
        "This notebook demonstrates how to structure your code for the DAVIS contest\r\n",
        "by means of an end-to-end example using\r\n",
        "the\r\n",
        "[Keras](https://keras.io/)\r\n",
        "deep learning framework.\r\n",
        "See [this colab notebook](http://wandb.me/davis-starter-pt)\r\n",
        "for the same in PyTorch/PyTorch Lightning.\r\n",
        "\r\n",
        "You should feel free to make use of the code here and in\r\n",
        "[the contest repo](https://github.com/wandb/davis-contest)\r\n",
        "(installed via `pip` below and imported as `contest`)\r\n",
        "to build your data engineering and model training pipelines,\r\n",
        "but that's not strictly necessary to compete in the contest.\r\n",
        "All that you need to do is produce your results\r\n",
        "in an appropriately-formatted\r\n",
        "Weights & Biases [Artifact](https://docs.wandb.ai/artifacts),\r\n",
        "as described below,\r\n",
        "and follow the instructions in the\r\n",
        "[submission notebook](http://wandb.me/davis-submit)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHSX1I__VJSW"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!pip install git+https://github.com/wandb/davis-contest.git#egg=contest[keras]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxaxWQGxU9pi"
      },
      "source": [
        "import os \r\n",
        "\r\n",
        "import wandb\r\n",
        "\r\n",
        "import contest\r\n",
        "from contest.utils import clips, paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtUNLAmqze-T"
      },
      "source": [
        "## 0️⃣ Create a Weights & Biases account if you don't have one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHogIiFVhXjv"
      },
      "source": [
        "[Weights & Biases](https://wandb.ai/site)\r\n",
        "is a developer toolkit for machine learning --\r\n",
        "kind of like GitHub, but specialized\r\n",
        "to the particular problems that come up in machine learning.\r\n",
        "\r\n",
        "We'll be using it throughout the contest\r\n",
        "to organize datasets,\r\n",
        "track models during training,\r\n",
        "and evaluate model performance for submission.\r\n",
        "\r\n",
        "Run the cell below to either log in to Weights & Biases\r\n",
        "or create a new account.\r\n",
        "If you're participating in the contest,\r\n",
        "make sure to sign up under your company email address."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lokBIxspYcTx"
      },
      "source": [
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoTHl37XxAS9"
      },
      "source": [
        "## 1️⃣ Download the training data from Weights & Biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQkaVxGJYgyP"
      },
      "source": [
        "First, we need to download the training data\r\n",
        "onto the machine we're using.\r\n",
        "This same code will also work to pull the data down onto\r\n",
        "your own machines.\r\n",
        "\r\n",
        "The data is stored as a Weights & Biases\r\n",
        "[Artifact](https://docs.wandb.ai/artifacts).\r\n",
        "The Artifacts system allows you\r\n",
        "to track the large binary files that are inputs to\r\n",
        "and outputs of machine learning experiments.\r\n",
        "Your final submission in the contest\r\n",
        "will be in the form of an Artifact.\r\n",
        "Check out [this video tutorial](http://wandb.me/artifacts-video)\r\n",
        "to learn more about how to use Artifacts,\r\n",
        "or read the docs [here](https://docs.wandb.ai/artifacts/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXF0H0kpVRAq"
      },
      "source": [
        "# picking out the training data artifact by name\r\n",
        "\r\n",
        "entity = \"charlesfrye\"  # artifacts are associated with an entity -- s user or team\r\n",
        "project = \"davis\"  # artifacts are associated with a project -- a collection of ML experiments\r\n",
        "split = \"train\"  # the train and val data are both stored in the same format\r\n",
        "tag = \"latest\"  # different versions of an Artifact have different tags\r\n",
        "\r\n",
        "training_data_artifact_id = os.path.join(entity, project, f\"davis2016-{split}\") + \":\" + tag\r\n",
        "training_data_artifact_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIqkbe9mYpwD"
      },
      "source": [
        "Calling `run.use_artifact` and then `.download()`\r\n",
        "during a script downloads the files in the Artifact,\r\n",
        "if they aren't already present locally.\r\n",
        "\r\n",
        "This cell contains the minimal code you need to get the training data.\r\n",
        "Below, we'll see how to integrate Artifacts into your pipeline more fully,\r\n",
        "so that you can, e.g., track which inputs a model was trained on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyTXkGNEU8wo"
      },
      "source": [
        "with wandb.init(project=project, job_type=\"download\") as run:\r\n",
        "  training_data_artifact = run.use_artifact(training_data_artifact_id)\r\n",
        "  training_data_dir = training_data_artifact.download()\r\n",
        "  print(\"\\ntraining data downloaded to \" + training_data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw9pYa8vxEGJ"
      },
      "source": [
        "### Dataset format and exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUaeG8gQYw35"
      },
      "source": [
        "You can view the training data\r\n",
        "in the format used by all of the datasets,\r\n",
        "including the test set,\r\n",
        "which will only be available at the end of the contest,\r\n",
        "[here](http://wandb.me/davis-train-data).\r\n",
        "A short description of that format follows.\r\n",
        "\r\n",
        "Every dataset artifact has, at the top-level directory,\r\n",
        "a file called `paths.json`,\r\n",
        "which contains information on the paths to data files in the artifact.\r\n",
        "\r\n",
        "These files are intended to be read as\r\n",
        "[pandas DataFrames](https://pandas.pydata.org/).\r\n",
        "The resulting columns will possibly include\r\n",
        "- `\"raw\"`, for the input image files\r\n",
        "- `\"annotation\"`, for the ground truth segmentation masks, as PNG files, and\r\n",
        "- `\"output\"`, for model predictions, for results saved as Artifacts.\r\n",
        "\r\n",
        "Note that the test set, when provided,\r\n",
        "will not have an `\"annotation\"` column,\r\n",
        "so make sure your model can run on just raw images!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NOK5vccYzJu"
      },
      "source": [
        "![data-artifact-format](https://i.imgur.com/WQIXC0O.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TOxeb2hY0Xb"
      },
      "source": [
        "However, every path will have, at the end, two elements:\r\n",
        "`{clip_name}/{12345}.jpg`\r\n",
        "where\r\n",
        "- `clip_name` is a string identifying the video clip to which the image belongs\r\n",
        "and\r\n",
        "- `12345` is a five-digit, [zero-filled](https://docs.python.org/3/library/stdtypes.html#str.zfill) number indicating the frame index of the image.\r\n",
        "\r\n",
        "\r\n",
        "The columns are assumed to be indexed by integers,\r\n",
        "and these integers are used\r\n",
        "to match `\"raw\"` and `\"annotation\"` in the starter code and\r\n",
        "to match `\"output\"` and `\"annotation\"`\r\n",
        "in the final submission evaluation code,\r\n",
        "which must be used for all submissions.\r\n",
        "\r\n",
        "See below for an example.\r\n",
        "\r\n",
        "The prefixes of paths are arbitrary and may have differing depths\r\n",
        "(the examples below have three directories,\r\n",
        "but the actual datasets may have a different number)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5ZAod-oY4sv"
      },
      "source": [
        "![paths-content](https://i.imgur.com/Bh7EKte.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOymsn4DY66e"
      },
      "source": [
        "For convenience, the data has also been packaged up into a\r\n",
        "Weights & Biases Dataset Visualizaton Table [here](http://wandb.me/davis-train-table).\r\n",
        "This format, pictured below, is convenient for exploring the data\r\n",
        "and getting to know it better.\r\n",
        "\r\n",
        "You can read more about DSviz Tables [here](https://docs.wandb.ai/datasets-and-predictions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LSykpnpY84n"
      },
      "source": [
        "![data-table-format](https://i.imgur.com/mliFzqc.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMuifb6FxJlB"
      },
      "source": [
        "## 2️⃣ Set up your data pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_YzbGgmZCTW"
      },
      "source": [
        "Now that the data is downloaded to the filesystem,\r\n",
        "we need to define a method for getting the data onto the GPU\r\n",
        "and into the model.\r\n",
        "\r\n",
        "This is much more complicated for big datasets,\r\n",
        "like this one, that can't fit inside the GPU\r\n",
        "comfortably alongside our model.\r\n",
        "\r\n",
        "In the [GitHub repo for this contest](https://github.com/wandb/davis-contest),\r\n",
        "we provide tools for loading data from disk using the\r\n",
        "[Keras](https://keras.io/)\r\n",
        "deep learning framework,\r\n",
        "which provides a neural network API\r\n",
        "and data pipeline functionality\r\n",
        "for [TensorFlow](https://www.tensorflow.org/).\r\n",
        "\r\n",
        "For more on using Keras with Weights & Biases,\r\n",
        "check out\r\n",
        "[this tutorial video](http://wandb.me/keras-video)\r\n",
        "and [colab notebook](http://wandb.me/keras-colab)\r\n",
        "or read the [W&B docs](https://docs.wandb.ai/integrations/keras)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfEaKFR9ZlQP"
      },
      "source": [
        "print(contest.keras.data.VidSegDatasetSequence.__doc__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CiQ8lyzZ5By"
      },
      "source": [
        "The tools provided load images without regard to which video they come from,\r\n",
        "and so it's difficult if not impossible to build a model that\r\n",
        "can make use of information over time.\r\n",
        "\r\n",
        "One easy win over this baseline would be to rewrite this data-loading code\r\n",
        "to load clips and then construct a model architecture that makes use of temporal sequence information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3ZHadq5LAQ1"
      },
      "source": [
        "### Splitting up the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxCJjb4jaAhK"
      },
      "source": [
        "The small size of this dataset,\r\n",
        "relative to the difficulty of the task,\r\n",
        "increases the danger of over-fitting.\r\n",
        "\r\n",
        "To help track this during training,\r\n",
        "we'll split off some data into a holdout set\r\n",
        "and track our performance on that data.\r\n",
        "\r\n",
        "But we can't just randomly subsample specific frames,\r\n",
        "the way holdout sets are constructed in image datasets.\r\n",
        "That's because certain frames come from the same video,\r\n",
        "or _clip_, and holding out, say,\r\n",
        "every third frame from each clip\r\n",
        "doesn't prevent over-fitting nearly as effectively\r\n",
        "as holding out a third of the clips.\r\n",
        "\r\n",
        "To make working with clips easier,\r\n",
        "we provide utilities for splitting datasets\r\n",
        "at the level of clips.\r\n",
        "Use these tools as a blueprint\r\n",
        "for setting up your own tools that are \"clip-aware\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-TLlyygKxZg"
      },
      "source": [
        "print(clips.split_on_clips.__doc__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4a4iHs1Kss6"
      },
      "source": [
        "The code below will create a random split into training and holdout validation data,\r\n",
        "at the level of clips, and then log the result\r\n",
        "to a Weights & Biases artifact.\r\n",
        "Notice the addition of a `paths.json` file to the artifact,\r\n",
        "so that it matches the format of other dataset artifacts.\r\n",
        "\r\n",
        "`log_datasplit_artifact` demonstrates two steps needed to register an artifact on Weights & Biases:\r\n",
        "1. `add_file`s or `add_dir`s to the artifact to build it, and then\r\n",
        "2. upload the artifact to W&B servers using `run.log_artifact`.\r\n",
        "\r\n",
        "See the [documentation](https://docs.wandb.ai/artifacts/api)\r\n",
        "or the [tutorial video](http://wandb.me/artifacts-video) for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkak4W9n_W1N"
      },
      "source": [
        "def log_holdout_split(data_artifact, train_split_df, holdout_split_df):\r\n",
        "  log_datasplit_artifact(data_artifact, train_split_df, \"train\")\r\n",
        "  log_datasplit_artifact(data_artifact, holdout_split_df, \"holdout\")\r\n",
        "\r\n",
        "\r\n",
        "def log_datasplit_artifact(data_artifact, split_df, splitname, folder=\"wandb\"):\r\n",
        "  dataset_artifact = wandb.Artifact(name=f\"davis2016-split-{splitname}\", type=\"split-data\")\r\n",
        "  path = os.path.join(folder, splitname + \".json\")\r\n",
        "  split_df.to_json(path)\r\n",
        "  dataset_artifact.add_file(path, \"paths.json\")\r\n",
        "\r\n",
        "  wandb.run.log_artifact(dataset_artifact)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQqBg6VQENhV"
      },
      "source": [
        "config = {\"training_fraction\": 0.8}\r\n",
        "\r\n",
        "with wandb.init(project=project,\r\n",
        "                job_type=\"split-data\", config=config) as run:\r\n",
        "  training_data_artifact = run.use_artifact(training_data_artifact_id)\r\n",
        "  paths_df = paths.artifact_paths(training_data_artifact)\r\n",
        "\r\n",
        "  training_paths_df, holdout_paths_df = clips.split_on_clips(paths_df)\r\n",
        "  log_holdout_split(training_data_artifact,\r\n",
        "                    training_paths_df,\r\n",
        "                    holdout_paths_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHD28vXUaIJa"
      },
      "source": [
        "Notice also that this code makes use of the `training_data_artifact` with `run.use_artifact`.\r\n",
        "\r\n",
        "Logging where data came from\r\n",
        "(while simultaneously downloading it if need be!)\r\n",
        "makes it easier to understand and reproduce your work later,\r\n",
        "track down bugs or identify the cause of model regressions,\r\n",
        "and otherwise understand how the data influenced your model.\r\n",
        "\r\n",
        "For example, if you check the Artifacts tab\r\n",
        "on the run page for this run on Weights & Biases\r\n",
        "(see the auto-generated link produced when you run the cell below;\r\n",
        "the Artifacts tab is accessed by clicking the icon\r\n",
        "that looks like three hockey pucks in a stack),\r\n",
        "you can see which artifacts were used during the run\r\n",
        "and which were produced by it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwg9sJFXaKRH"
      },
      "source": [
        "![artifact-io](https://i.imgur.com/Q7HzzF4.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTm40hOZaMNY"
      },
      "source": [
        "This information is collated into a graph, as pictured below,\r\n",
        "that can be used to survey the entire pipeline of your project all at once.\r\n",
        "Use the Explode button to track individual runs and artifacts.\r\n",
        "\r\n",
        "This graph is\r\n",
        "accessible via the Graph View tab on an individual artifact's page\r\n",
        "(see [here](http://wandb.me/davis-artifacts-graph-eg) for an example).\r\n",
        "\r\n",
        "The Graph View is also covered in the [video tutorial for Artifacts](http://wandb.me/artifacts-video)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM9YEILqaOyY"
      },
      "source": [
        "![artifacts-dag](https://i.imgur.com/F5sQIjz.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHMtdiGraRAA"
      },
      "source": [
        "## 3️⃣ Define a model and train it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcvSDhvzaSmV"
      },
      "source": [
        "Now that the data pipeline is set up,\r\n",
        "we can define a model that consumes the data\r\n",
        "and learns the task.\r\n",
        "\r\n",
        "It will need to take in images of arbitrary shape\r\n",
        "and then return outputs of the same shape,\r\n",
        "with values between 0 and 1,\r\n",
        "with high values corresponding to\r\n",
        "pixels that are more likely to be a part of the segmentation mask.\r\n",
        "\r\n",
        "This notebook demonstrates the absolute simplest model\r\n",
        "that can be applied to this data:\r\n",
        "a spatial convolution that looks at only one pixel at a time.\r\n",
        "This is fed into a `sigmoid` nonlinearity\r\n",
        "so that the output values are normalized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab2u0qDYvUXE"
      },
      "source": [
        "### Model Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI0BQGk9pEkc"
      },
      "source": [
        "import tensorflow.keras as keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZFcI2PoaWu_"
      },
      "source": [
        "This model uses the [`keras.Sequential`](https://keras.io/guides/sequential_model/) API,\r\n",
        "which is simpler to use.\r\n",
        "\r\n",
        "For more flexibility, check out the\r\n",
        "[`keras.Functional`](https://keras.io/guides/functional_api/) API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjVPNJaQG8tj"
      },
      "source": [
        "def make_model(config):\r\n",
        "  model = keras.Sequential(name=config.name)\r\n",
        "  \r\n",
        "  model.add(keras.layers.InputLayer(input_shape=(None, None, 3)))\r\n",
        "  model.add(keras.layers.experimental.preprocessing.Rescaling(1 / 255.))\r\n",
        "  model.add(keras.layers.Conv2D(1, kernel_size=1))\r\n",
        "  model.add(keras.layers.Activation(\"sigmoid\"))\r\n",
        "  \r\n",
        "  model.compile(optimizer=\"sgd\", loss=\"bce\")\r\n",
        "\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF4sq2BrKaZk"
      },
      "source": [
        "### Training Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECHhHtYPaxYT"
      },
      "source": [
        "The cell below uses the Keras `.fit` API to train the model. The `.fit` API orchestrates the \"plumbing\" tasks,\r\n",
        "like ensuring data is handed off to the model correctly.\r\n",
        "\r\n",
        "We also use the Weights & Biases integration with Keras,\r\n",
        "[`wandb.keras.Callback`](https://docs.wandb.ai/integrations/keras),\r\n",
        "to track training and log the run to W&B.\r\n",
        "Head to the run page\r\n",
        "(the link appears once you execute the cell below)\r\n",
        "to watch this information come in live\r\n",
        "or review it afterwards --\r\n",
        "system metrics, loss metrics, and more all get logged without any extra effort."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hia3dlPkefU1"
      },
      "source": [
        "model_artifact_name = \"dummy-baseline-keras\"\r\n",
        "\r\n",
        "config = {\"batch_size\": 32,\r\n",
        "          \"max_epochs\": 1,\r\n",
        "          \"name\": model_artifact_name}\r\n",
        "\r\n",
        "with wandb.init(project=project, config=config, job_type=\"train\") as run:\r\n",
        "\r\n",
        "  config = wandb.config\r\n",
        "  training_data_artifact = run.use_artifact(training_data_artifact_id)\r\n",
        "  training_data_artifact.download()\r\n",
        "\r\n",
        "  trainsplit_artifact = run.use_artifact(\"davis2016-split-train:latest\")\r\n",
        "  trainsplit_paths = paths.get_paths(trainsplit_artifact)\r\n",
        "\r\n",
        "  holdoutsplit_artifact = run.use_artifact(\"davis2016-split-holdout:latest\")\r\n",
        "  holdoutsplit_paths = paths.get_paths(holdoutsplit_artifact)\r\n",
        "\r\n",
        "  training_data = contest.keras.data.VidSegDatasetSequence(\r\n",
        "    trainsplit_paths[\"raw\"], trainsplit_paths[\"annotation\"], batch_size=config.batch_size)\r\n",
        "  holdout_data = contest.keras.data.VidSegDatasetSequence(\r\n",
        "    holdoutsplit_paths[\"raw\"], holdoutsplit_paths[\"annotation\"], batch_size=config.batch_size)\r\n",
        "\r\n",
        "  model = make_model(config=config)\r\n",
        "\r\n",
        "  model.fit(training_data, epochs=config.max_epochs,\r\n",
        "            validation_data=holdout_data,\r\n",
        "            callbacks=[wandb.keras.WandbCallback()]\r\n",
        "  )\r\n",
        "\r\n",
        "  wandb.config[\"nparams\"] = contest.keras.profile.count_params(model)\r\n",
        "  wandb.config[\"nflops\"] = contest.keras.profile.count_flops(\r\n",
        "    model, training_data[0][0])\r\n",
        "\r\n",
        "  model_artifact_id = contest.keras.utils.save_model_to_artifact(\r\n",
        "    os.path.join(wandb.run.dir, \"model-best.h5\"), model_artifact_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPj86A4Ma1Tg"
      },
      "source": [
        "Two things to point out here:\r\n",
        "1. Hyperparameters for the run are stored in the `config` dictionary at the top,\r\n",
        "which is passed to `wandb.init`. That way, the hyperparameter values are logged to W&B. These hyperparameters are then accessed using the `wandb.config` attribute. That way, you can be sure the logged values are the same as the values being used.\r\n",
        "2. Added to the config later, we have `nflops` and `nparams`, calculated using the `keras.profile` tools provided for the contest. While you don't need to track this during training, this information _must_ be included with your submission and be underneath the limits in the contest description,\r\n",
        "or else the submission is invalid.\r\n",
        "If your model's flops or parameters cannot be counted with the methods we provide, you're responsible for ensuring they are counted correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7gj0eF8yc2S"
      },
      "source": [
        "## 3️⃣ Run your model on the evaluation data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oleltn0vzS6o"
      },
      "source": [
        "As the contest runs,\r\n",
        "you can submit your performance on the validation data\r\n",
        "to be included on the leaderboard.\r\n",
        "Final standings will be determined based on the test set,\r\n",
        "which will be released,\r\n",
        "without labels,\r\n",
        "in the last 72 hours of the contest.\r\n",
        "It's a well-known phenomenon that the best performers on validation data\r\n",
        "are not always the best performers on new test data,\r\n",
        "even in restricted settings like Kaggle competitions.\r\n",
        "The difficulty of the task and the heterogeneity of video data\r\n",
        "make this especially likely for this contest,\r\n",
        "as is common in production machine learning.\r\n",
        "\r\n",
        "In order to provide a framework-independent format for results,\r\n",
        "that can be used for both validation and test data,\r\n",
        "the submission generation process has been split into two steps:\r\n",
        "1. Execute the model on the evaluation data, producing a \"result\" artifact with a specific structure, described below.\r\n",
        "2. During the contest, run the [submission notebook](http://wandb.me/davis-submit) to get a final score on the validation set.\r\n",
        "\r\n",
        "Note that the validation data contains labels,\r\n",
        "but the test data will not!\r\n",
        "Take care to write your result generation code\r\n",
        "so that it will run even if no annotations are provided.\r\n",
        "\r\n",
        "We provide starter code for both PyTorch and Keras,\r\n",
        "using the simple dataloaders provided for the training loop above.\r\n",
        "If you change the data pipeline,\r\n",
        "you may need to write your own code here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5xg9fMtO1OJ"
      },
      "source": [
        "evaluation_artifact_name = os.path.join(entity, project, \"davis2016-val\" +\":\" + tag)\n",
        "\n",
        "model_tag = \"latest\"\n",
        "\n",
        "result_artifact_name = model_artifact_name + \"-result\"\n",
        "\n",
        "output_dir = os.path.join(\"outputs\")\n",
        "!rm -rf output_dir\n",
        "!mkdir -p {output_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkNEH5zOOIJ-"
      },
      "source": [
        "with wandb.init(project=project, job_type=\"run-val\") as run:\r\n",
        "  evaluation_data_artifact = run.use_artifact(evaluation_artifact_name)\r\n",
        "  evaluation_data_paths = paths.artifact_paths(evaluation_data_artifact)\r\n",
        "\r\n",
        "  evaluation_dataset = contest.keras.data.VidSegDatasetSequence(\r\n",
        "    evaluation_data_paths[\"raw\"])\r\n",
        "  num_images = len(evaluation_dataset.image_paths)\r\n",
        "\r\n",
        "  model = contest.keras.utils.load_model_from_artifact(\r\n",
        "    model_artifact_name + \":\" + model_tag)\r\n",
        "\r\n",
        "  print(\"\\n\")\r\n",
        "  nparams = contest.keras.profile.count_params(model)\r\n",
        "  nflops = contest.keras.profile.count_flops(model, evaluation_dataset[0])\r\n",
        "\r\n",
        "  profiling_info = {\"nparams\": nparams, \"nflops\": nflops}\r\n",
        "  wandb.log(profiling_info)\r\n",
        "\r\n",
        "  output_paths = contest.keras.evaluate.run(\r\n",
        "    model, evaluation_dataset, num_images, output_dir)\r\n",
        "\r\n",
        "  result_artifact = contest.evaluate.make_result_artifact(\r\n",
        "    output_paths, result_artifact_name, metadata=profiling_info)\r\n",
        "  run.log_artifact(result_artifact)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyDmZ_UFcErx"
      },
      "source": [
        "Check out the page for the result artifact associated with this run\r\n",
        "(link appears above after executing the cell)\r\n",
        "in order to see an example of a formatted result.\r\n",
        "\r\n",
        "A result artifact looks much like a dataset artifact\r\n",
        "-- it has a `paths.json` file,\r\n",
        "along with files that are pointed to by the contents of that file --\r\n",
        "but it need only contain a single key: `\"output\"`.\r\n",
        "The output files are black and white PNG files with unsigned 8-bit pixel values between 0 and 255 that represent the model's confidence that a given pixel\r\n",
        "in the image is part of the segmentation mask."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSC_xH6lcGGN"
      },
      "source": [
        "The result artifact should also include count for both FLOPs and parameters,\r\n",
        "as above.\r\n",
        "See the discussion above in the model training code for details.\r\n",
        "If your result does not have counts for the FLOPs and parameters,\r\n",
        "it will be declared invalid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xQDEuHMzNjp"
      },
      "source": [
        "## 4️⃣ Submit your results to the leaderboard on Weights & Biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGn2Y9wUU2W9"
      },
      "source": [
        "Once you've run an evaluation job like the one above and produced a results artifact,\r\n",
        "you're almost ready to submit to the contest.\r\n",
        "\r\n",
        "Head over to [this notebook](http://wandb.me/davis-submit) for the last two steps."
      ]
    }
  ]
}